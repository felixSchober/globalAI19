{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, \n",
    "            out_channels=32, \n",
    "            kernel_size=3, \n",
    "            stride=1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, \n",
    "            out_channels=64, \n",
    "            kernel_size=3, \n",
    "            stride=1\n",
    "        )\n",
    "        self.dropout_25 = nn.Dropout2d(0.25)\n",
    "        self.dropout_50 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        #in_features = \n",
    "        \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=9216, \n",
    "            out_features=128)\n",
    "        \n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=128, \n",
    "            out_features=10)\n",
    "        \n",
    "    def forward_verbose(self, x):\n",
    "        print(f'[Initial]{x.shape}')\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        print(f'[1st Conv]{x.shape}')\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        print(f'[2nd Conv]{x.shape}')\n",
    "\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "        print(f'[MaxPool]{x.shape}')\n",
    "\n",
    "        x = self.dropout_25(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(f'[Flatten]{x.shape}')\n",
    "\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        print(f'[FC Layer 1]{x.shape}')\n",
    "\n",
    "        \n",
    "        x = self.dropout_50(x)\n",
    "        x = self.fc2(x)\n",
    "        print(f'[FC Layer 1]{x.shape}')\n",
    "\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        print(f'[Softmax]{x.shape}')\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.dropout_25(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        \n",
    "        x = self.dropout_50(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "sample = torch.Tensor(np.ones((1, 1, 28, 28)))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Initial]torch.Size([1, 1, 28, 28])\n",
      "[1st Conv]torch.Size([1, 32, 26, 26])\n",
      "[2nd Conv]torch.Size([1, 64, 24, 24])\n",
      "[MaxPool]torch.Size([1, 64, 12, 12])\n",
      "[Flatten]torch.Size([1, 9216])\n",
      "[FC Layer 1]torch.Size([1, 128])\n",
      "[FC Layer 1]torch.Size([1, 10])\n",
      "[Softmax]torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3919, -2.2580, -2.2217, -2.5686, -2.1988, -2.3308, -2.2612, -2.1705,\n",
       "         -2.3731, -2.3104]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward_verbose(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, epoch):\n",
    "    # set model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # iterate over data\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # perform forward pass\n",
    "        output = model.forward(data)\n",
    "        \n",
    "        # calculate the loss based on the forward pass and the true labels\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # do the backpropagation step\n",
    "        # this calculates the gradients for each parameter\n",
    "        # each parameter x (eg. weight) receives a x.grad value\n",
    "        loss.backward()\n",
    "        \n",
    "        # adjust the parameters based on gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print current state every 1000 steps\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_data.dataset)}] Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data_torch\n",
    "def run():\n",
    "    epochs = 15\n",
    "    model = ConvNet()\n",
    "    train_data, test_data = load_data_torch()\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_data, optimizer, epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/60000] Loss: 2.294050693511963\n",
      "Epoch: 1 [640/60000] Loss: 3.1388297080993652\n",
      "Epoch: 1 [1280/60000] Loss: 0.4638303220272064\n",
      "Epoch: 1 [1920/60000] Loss: 0.47239527106285095\n",
      "Epoch: 1 [2560/60000] Loss: 0.43595775961875916\n",
      "Epoch: 1 [3200/60000] Loss: 0.6079074144363403\n",
      "Epoch: 1 [3840/60000] Loss: 0.26333513855934143\n",
      "Epoch: 1 [4480/60000] Loss: 0.24814258515834808\n",
      "Epoch: 1 [5120/60000] Loss: 0.38512298464775085\n",
      "Epoch: 1 [5760/60000] Loss: 0.21260781586170197\n",
      "Epoch: 1 [6400/60000] Loss: 0.15531380474567413\n",
      "Epoch: 1 [7040/60000] Loss: 0.07880792766809464\n",
      "Epoch: 1 [7680/60000] Loss: 0.18957430124282837\n",
      "Epoch: 1 [8320/60000] Loss: 0.1408403217792511\n",
      "Epoch: 1 [8960/60000] Loss: 0.26074495911598206\n",
      "Epoch: 1 [9600/60000] Loss: 0.23507148027420044\n",
      "Epoch: 1 [10240/60000] Loss: 0.21115614473819733\n",
      "Epoch: 1 [10880/60000] Loss: 0.11111243814229965\n",
      "Epoch: 1 [11520/60000] Loss: 0.11354387551546097\n",
      "Epoch: 1 [12160/60000] Loss: 0.2026374489068985\n",
      "Epoch: 1 [12800/60000] Loss: 0.15429642796516418\n",
      "Epoch: 1 [13440/60000] Loss: 0.18446722626686096\n",
      "Epoch: 1 [14080/60000] Loss: 0.195431187748909\n",
      "Epoch: 1 [14720/60000] Loss: 0.17446278035640717\n",
      "Epoch: 1 [15360/60000] Loss: 0.17019078135490417\n",
      "Epoch: 1 [16000/60000] Loss: 0.2978108823299408\n",
      "Epoch: 1 [16640/60000] Loss: 0.10478750616312027\n",
      "Epoch: 1 [17280/60000] Loss: 0.0865267887711525\n",
      "Epoch: 1 [17920/60000] Loss: 0.1764964461326599\n",
      "Epoch: 1 [18560/60000] Loss: 0.0822165459394455\n",
      "Epoch: 1 [19200/60000] Loss: 0.2513815462589264\n",
      "Epoch: 1 [19840/60000] Loss: 0.2436499297618866\n",
      "Epoch: 1 [20480/60000] Loss: 0.27137067914009094\n",
      "Epoch: 1 [21120/60000] Loss: 0.07931999862194061\n",
      "Epoch: 1 [21760/60000] Loss: 0.09340336918830872\n",
      "Epoch: 1 [22400/60000] Loss: 0.14285899698734283\n",
      "Epoch: 1 [23040/60000] Loss: 0.12026114761829376\n",
      "Epoch: 1 [23680/60000] Loss: 0.13130560517311096\n",
      "Epoch: 1 [24320/60000] Loss: 0.06288216263055801\n",
      "Epoch: 1 [24960/60000] Loss: 0.08549759536981583\n",
      "Epoch: 1 [25600/60000] Loss: 0.04878925532102585\n",
      "Epoch: 1 [26240/60000] Loss: 0.05950171500444412\n",
      "Epoch: 1 [26880/60000] Loss: 0.3275010585784912\n",
      "Epoch: 1 [27520/60000] Loss: 0.09937584400177002\n",
      "Epoch: 1 [28160/60000] Loss: 0.1801222562789917\n",
      "Epoch: 1 [28800/60000] Loss: 0.05120117589831352\n",
      "Epoch: 1 [29440/60000] Loss: 0.04711541905999184\n",
      "Epoch: 1 [30080/60000] Loss: 0.03559191897511482\n",
      "Epoch: 1 [30720/60000] Loss: 0.20167136192321777\n",
      "Epoch: 1 [31360/60000] Loss: 0.03573070093989372\n",
      "Epoch: 1 [32000/60000] Loss: 0.08501127362251282\n",
      "Epoch: 1 [32640/60000] Loss: 0.2432537078857422\n",
      "Epoch: 1 [33280/60000] Loss: 0.09707187116146088\n",
      "Epoch: 1 [33920/60000] Loss: 0.029834767803549767\n",
      "Epoch: 1 [34560/60000] Loss: 0.036915380507707596\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-8ba5463ca6eb>\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-b46e8a41c3b1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_data, optimizer, epoch)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# this calculates the gradients for each parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# each parameter x (eg. weight) receives a x.grad value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# adjust the parameters based on gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
